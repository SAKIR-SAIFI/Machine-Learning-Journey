{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5728ae8d-7540-486e-be61-821cee12e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---\n",
    "\n",
    "# ### **1. Evaluation Metrics for Classification Problems**\n",
    "# Classification problems predict a discrete label (e.g., spam or not spam).\n",
    "\n",
    "# #### (a) **Confusion Matrix**\n",
    "# A table that summarizes the performance of a classification model.\n",
    "# - **True Positive (TP)**: Correctly predicted positive class.\n",
    "# - **True Negative (TN)**: Correctly predicted negative class.\n",
    "# - **False Positive (FP)**: Incorrectly predicted as positive.\n",
    "# - **False Negative (FN)**: Incorrectly predicted as negative.\n",
    "\n",
    "# |                  | Predicted Positive | Predicted Negative |\n",
    "# |------------------|--------------------|--------------------|\n",
    "# | **Actual Positive** | TP                 | FN                 |\n",
    "# | **Actual Negative** | FP                 | TN                 |\n",
    "\n",
    "# ---\n",
    "\n",
    "# #### (b) **Metrics Derived from the Confusion Matrix**\n",
    "# 1. **Accuracy**: \n",
    "#    - Proportion of correctly predicted instances.\n",
    "#    - Formula:  \n",
    "#          Accuracy= (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "\n",
    "# 2. **Precision** (or Positive Predictive Value): \n",
    "#    - How many of the predicted positives are actually positive.\n",
    "#    - Formula:  \n",
    "#          Precision= TP/(TP+FP)\n",
    "\n",
    "\n",
    "# 3. **Recall** (or Sensitivity or True Positive Rate):\n",
    "#    - How many of the actual positives were correctly predicted.\n",
    "#    - Formula:  \n",
    "#          Recall= TP/(TP+FN)\n",
    "\n",
    "\n",
    "# 4. **F1-Score**: \n",
    "#    - Harmonic mean of Precision and Recall.\n",
    "#    - Formula:  \n",
    "#          F1=2×((Precision×Recall)/Precision+Recall)\n",
    "\n",
    "\n",
    "# 5. **Specificity** (or True Negative Rate): \n",
    "#    - Proportion of actual negatives correctly identified.\n",
    "#    - Formula:  \n",
    "#          Specificity= TN/(TN+FP)\n",
    "\n",
    "# ---\n",
    "\n",
    "# #### (c) **Other Metrics**\n",
    "# 1. **ROC-AUC Score**:\n",
    "#    - Evaluates the model's ability to distinguish between classes at different thresholds.\n",
    "#    - **ROC Curve**: Plots True Positive Rate vs. False Positive Rate.\n",
    "#    - **AUC (Area Under the Curve)**: Measures the area under the ROC curve (higher is better).\n",
    "\n",
    "# 2. **Log Loss**:\n",
    "#    - Penalizes incorrect predictions by calculating the difference between predicted probabilities and actual labels.\n",
    "#    - Formula:  \n",
    "#      \\[\n",
    "#      \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "#      \\]\n",
    "#      Where \\( y_i \\) is the actual label and \\( p_i \\) is the predicted probability.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **2. Evaluation Metrics for Regression Problems**\n",
    "# Regression problems predict continuous values (e.g., house prices).\n",
    "\n",
    "# 1. **Mean Absolute Error (MAE)**:\n",
    "#    - Average absolute difference between predicted and actual values.\n",
    "#    - Formula:  \n",
    "#        MAE = 1/N((N∑i=1)(yi−y^i)\n",
    "\n",
    "\n",
    "# 2. **Mean Squared Error (MSE)**:\n",
    "#    - Average squared difference between predicted and actual values.\n",
    "#    - Formula:  \n",
    "#          MSE= 1/N((N∑i=1)sqr(yi−y^i))\n",
    "\n",
    "\n",
    "# 3. **Root Mean Squared Error (RMSE)**:\n",
    "#    - Square root of MSE, giving error in the same unit as the target variable.\n",
    "#    - Formula:  \n",
    "#          RMSE = sqrt(MSE)\n",
    "\n",
    "\n",
    "# 4. **R-squared (sqr(R))**:\n",
    "#    - Proportion of variance in the target variable explained by the model.\n",
    "#    - Formula:  \n",
    "#      sqr(R) = 1− (Sum of Squared Residuals (SSR)/Total Sum of Squares (TSS))\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Key Points to Remember**\n",
    "# - Use **Accuracy** when class distributions are balanced.\n",
    "# - Use **Precision** and **Recall** when false positives or false negatives are critical.\n",
    "# - For regression, lower values of **MAE**, **MSE**, or **RMSE** indicate better performance.\n",
    "# - **R-squared** closer to 1 means a better fit.\n",
    "\n",
    "# ---\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5a5ded-13e4-4858-914e-5685f3c815fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go through some examples of evaluation metrics in Python. \n",
    "# We'll use Scikit-learn, a popular machine learning library, for implementation.\n",
    "\n",
    "# Setup\n",
    "    # First, install Scikit-learn if you haven’t already:\n",
    "    # pip install scikit-learn\n",
    "# Then, import the necessary libraries:\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    confusion_matrix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f3d703-ddf7-49b0-82ad-8f8f4d670176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [1 4]]\n",
      "\n",
      "Accuracy: 0.8\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1-Score: 0.8\n",
      "ROC-AUC Score: 0.9600000000000001\n"
     ]
    }
   ],
   "source": [
    "# 1. Classification Metrics Example\n",
    "# We'll use a toy example with predicted and actual labels.\n",
    "# Code:\n",
    "# Actual labels\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "\n",
    "# Predicted labels\n",
    "y_pred = [1, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1-Score:\", f1)\n",
    "/\n",
    "# ROC-AUC Score\n",
    "# Assuming we have prediction probabilities for the positive class\n",
    "y_probs = [0.9, 0.1, 0.8, 0.7, 0.2, 0.4, 0.1, 0.6, 0.9, 0.3]\n",
    "roc_auc = roc_auc_score(y_true, y_probs)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Output Explanation:\n",
    "    # The Confusion Matrix will show counts for TP, TN, FP, and FN.\n",
    "    # Accuracy measures the overall correctness of the predictions.\n",
    "    # Precision focuses on the reliability of positive predictions.\n",
    "    # Recall tells how many actual positives were identified correctly.\n",
    "    # F1-Score balances Precision and Recall.\n",
    "    # ROC-AUC Score evaluates the performance across different thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e7aac5-4dfd-4dc8-9f4a-17604f4ab8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Absolute Error (MAE): 0.475\n",
      "Mean Squared Error (MSE): 0.2874999999999999\n",
      "Root Mean Squared Error (RMSE): 0.5361902647381803\n",
      "R-squared (R^2): 0.9605995717344754\n"
     ]
    }
   ],
   "source": [
    "# 2. Regression Metrics Example\n",
    "# Here’s how to compute metrics for regression problems.\n",
    "# Code:\n",
    "\n",
    "# Actual values\n",
    "y_true = [3.0, -0.5, 2.0, 7.0]\n",
    "\n",
    "# Predicted values\n",
    "y_pred = [2.5, 0.0, 2.1, 7.8]\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"\\nMean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# R-squared (R^2)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"R-squared (R^2):\", r2)\n",
    "\n",
    "# Output Explanation:\n",
    "    # MAE gives the average absolute difference between true and predicted values.\n",
    "    # MSE penalizes larger errors more than smaller errors due to squaring.\n",
    "    # RMSE provides error in the same units as the target variable.\n",
    "    # R-squared shows how well the model explains the variance in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd00e9-791c-4f07-819a-9b724d118f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
